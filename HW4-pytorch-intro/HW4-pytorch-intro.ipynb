{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUbEmuvZJxlI"
   },
   "source": [
    "# Homework 4 - PyTorch Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efS07mO7J6AR"
   },
   "source": [
    "Please run the whole notebook with your code and submit the `.ipynb` file that includes your answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xDkwBg8LKQ_"
   },
   "source": [
    " ## Question 1 -- matrix multiplication\n",
    "\n",
    "Implement the following mathematical operation on both the CPU and GPU (use Google Colab or another cloud service if you don't have a GPU in your computer). Print:\n",
    "\n",
    "a) which type of GPU card you have and \n",
    "\n",
    "b) show the computation time for both CPU and GPU (using PyTorch). \n",
    "\n",
    "c) How much % fast is the GPU? \n",
    "\n",
    " The operation to implement is the dot product $C = B * A^T$\n",
    "\n",
    " whereby $A$ is a random matrix of size $30,000 \\times 1000$ and $B$ is a random matrix of size $3000 \\times 1000$. In addition to the required information asked above:\n",
    " \n",
    " d) please also print the resulting two $C$ matrices (they should be the same btw). \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "BINvhm-PLKak",
    "outputId": "b1692791-9109-41ef-c285-af52d5a93365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 26 06:03:18 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P0    31W /  70W |   1609MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "The computational time for CPU is 1.5670208930969238s\n",
      "The computational time for GPU is 0.03232550621032715s\n",
      "The gpu is 4847.629865101082% fast than the cpu\n",
      "tensor([[264.4876, 258.7579, 261.7922,  ..., 254.2617, 255.5585, 252.3149],\n",
      "        [250.6654, 245.7712, 244.9686,  ..., 243.8595, 250.3475, 245.5152],\n",
      "        [255.5456, 250.8218, 250.4889,  ..., 254.5170, 252.5769, 248.6868],\n",
      "        ...,\n",
      "        [250.9920, 249.8650, 251.3684,  ..., 243.2682, 245.9193, 240.8705],\n",
      "        [250.8901, 244.1719, 250.3598,  ..., 243.4244, 244.3190, 241.5399],\n",
      "        [261.9029, 260.9989, 258.8348,  ..., 253.3006, 259.4485, 255.9212]])\n",
      "tensor([[264.4877, 258.7578, 261.7922,  ..., 254.2618, 255.5585, 252.3150],\n",
      "        [250.6656, 245.7713, 244.9686,  ..., 243.8597, 250.3475, 245.5152],\n",
      "        [255.5456, 250.8218, 250.4888,  ..., 254.5169, 252.5769, 248.6869],\n",
      "        ...,\n",
      "        [250.9918, 249.8651, 251.3684,  ..., 243.2683, 245.9192, 240.8707],\n",
      "        [250.8901, 244.1718, 250.3600,  ..., 243.4244, 244.3189, 241.5400],\n",
      "        [261.9028, 260.9988, 258.8347,  ..., 253.3004, 259.4485, 255.9213]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# implement solution here\n",
    "import time\n",
    "import torch\n",
    "# a) which type of GPU card you have and\n",
    "!nvidia-smi\n",
    "# b) show the computation time for both CPU and GPU (using PyTorch).\n",
    "A = torch.rand(30000, 1000)\n",
    "A_T = torch.transpose(A, 0, 1)\n",
    "B = torch.rand(3000, 1000)\n",
    "\n",
    "cpu_start = time.time()\n",
    "C_cpu = torch.mm(B, A_T)\n",
    "cpu_end = time.time()\n",
    "cpu_time = cpu_end-cpu_start\n",
    "print(\"The computational time for CPU is {}s\".format(cpu_time))\n",
    "\n",
    "gpu_start = time.time()\n",
    "C_gpu = torch.mm(B.cuda(), A_T.cuda())\n",
    "gpu_end = time.time()\n",
    "gpu_time = gpu_end-gpu_start\n",
    "print(\"The computational time for GPU is {}s\".format(gpu_time))\n",
    "\n",
    "# c) How much % fast is the GPU?\n",
    "print(\"The gpu is {}% fast than the cpu\".format(cpu_time/gpu_time*100))\n",
    "\n",
    "# d) please also print the resulting two  C  matrices (they should be the same btw).\n",
    "print(C_cpu)\n",
    "print(C_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZJXmfT-yU3g"
   },
   "source": [
    "## Question 2 - grad\n",
    "\n",
    "\n",
    "Find the gradient (partial derivatives) of the function $g(w)$ below. \n",
    "\n",
    "Let  $w=[w_1,w_2]^T$\n",
    "\n",
    "Consider  $g(w)=2w_1w_2+w_2cos(w_1)$\n",
    "\n",
    "a) In PyTorch, compute:   $\\Delta_w g(w)$ \n",
    "\n",
    " and verify that $\\Delta_w g([\\pi,1])=[2,\\piâˆ’1]^T$ using the grad function, whereby the first position is the partial for $w_1$ and the second position is the partial for $w_2$. \n",
    "\n",
    "b) You can also write a function to manually calculate these partial derivatives! You can review your differential equations math at [here](https://www.wolframalpha.com/input/?i=derivative+y+cos%28x%29) and implement this is a second function below to verify that it comes to the same solution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "pLjz6_LKt4sc",
    "outputId": "8f22c876-5c8a-4157-91d5-0d83fbce10cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pytorch grad: [2.0, 5.2831854820251465]\n",
      "Manually calculate grad: [2.0, 5.2831854820251465]\n"
     ]
    }
   ],
   "source": [
    "# write your solution here\n",
    "import math\n",
    "# (a)\n",
    "def g(w):\n",
    "    return (2*w[0]*w[1]) + (w[1]*torch.cos(w[0]))\n",
    "\n",
    "input_z = (torch.tensor(math.pi, requires_grad=True), torch.tensor(1.0, requires_grad=True))\n",
    "output_z = g(input_z)\n",
    "output_z.backward()\n",
    "print(\"Using Pytorch grad: {}\".format([input_z[0].grad.item(), input_z[1].grad.item()]))\n",
    "\n",
    "# (b)\n",
    "def manually_calculate(w):\n",
    "    return (2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0]))\n",
    "manual_output_z = manually_calculate(input_z)\n",
    "print(\"Manually calculate grad: {}\".format([manual_output_z[0].item(), manual_output_z[1].item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJwP6ur8LKjD"
   },
   "source": [
    "## Question 3 - dance hit song prediction\n",
    "\n",
    "Implement logistic regression in PyTorch for the following dance hit song prediction training dataset: \n",
    "https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
    "\n",
    " * Input variables: a number of audio features (most already standardized so don't worry about that)\n",
    " * Target variable: Topclass1030: \n",
    "   * 1 means it was a top 10 hit song; \n",
    "   * 0 means it never went above top 30 position.\n",
    "\n",
    "This dataset is derived from my paper on dance hit song prediction, for full description of features have a look at https://arxiv.org/abs/1905.08076. \n",
    "\n",
    "Print the evolution of the loss every few epochs and train the model until it converges. \n",
    " \n",
    " After training the logistic regression model, calculate the prediction accuracy on the test set: \n",
    " https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "VyRP6bl8t4Wc",
    "outputId": "c4646c2e-936a-49b6-cbbe-b04b86dced70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9041625261306763, \n",
      "Epoch: 10, Loss: 0.6218525171279907, \n",
      "Epoch: 20, Loss: 0.528566300868988, \n",
      "Epoch: 30, Loss: 0.46163666248321533, \n",
      "Epoch: 40, Loss: 0.42029836773872375, \n",
      "Epoch: 50, Loss: 0.39436790347099304, \n",
      "Epoch: 60, Loss: 0.37743163108825684, \n",
      "Epoch: 70, Loss: 0.365932822227478, \n",
      "Epoch: 80, Loss: 0.35787269473075867, \n",
      "Epoch: 90, Loss: 0.35208699107170105, \n",
      "Epoch: 100, Loss: 0.34787097573280334, \n",
      "Epoch: 110, Loss: 0.34477487206459045, \n",
      "Epoch: 120, Loss: 0.3425094783306122, \n",
      "Epoch: 130, Loss: 0.34087684750556946, \n",
      "Epoch: 140, Loss: 0.3397316038608551, \n",
      "Epoch: 150, Loss: 0.33897864818573, \n",
      "Epoch: 160, Loss: 0.3385392427444458, \n",
      "Epoch: 170, Loss: 0.3383553624153137, \n",
      "Epoch: 180, Loss: 0.33838361501693726, \n",
      "Epoch: 190, Loss: 0.3385876417160034, \n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "csv = pd.read_csv(\"https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\")\n",
    "targets = csv['Topclass1030']\n",
    "inputs = csv.drop('Topclass1030', 1)\n",
    "\n",
    "# define logistic regression model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "    super(LogisticRegression, self).__init__()\n",
    "    # Set up the linear transform\n",
    "    self.linear = nn.Linear(input_size, num_classes)\n",
    "    # I do not yet include the sigmoid activation after the linear \n",
    "    # layer because our loss function will include this as you will see later\n",
    "\n",
    "  # Forward's sole argument is the input.\n",
    "  # input is of shape (batch_size, input_size)\n",
    "  def forward(self, x):\n",
    "    # Apply the linear transform.\n",
    "    # out is of shape (batch_size, num_classes). \n",
    "    out = self.linear(x)\n",
    "    out = torch.sigmoid(out)\n",
    "    # Softmax the out tensor to get a log-probability distribution\n",
    "    # over classes for each example.\n",
    "    return out\n",
    "\n",
    "# train model\n",
    "num_outputs = 1\n",
    "num_input_features = csv.shape[1] - 1\n",
    "logreg_clf = LogisticRegression(num_input_features, num_outputs)\n",
    "\n",
    "import torch \n",
    "lr_rate = 0.001  # alpha\n",
    "loss_function = nn.BCELoss() \n",
    "# SGD: stochastic gradient descent is used to train/fit the model\n",
    "optimizer = torch.optim.SGD(logreg_clf.parameters(), lr=lr_rate)\n",
    "\n",
    "import numpy as np \n",
    "#training loop:\n",
    "epochs = 200 #how many times we go through the training set\n",
    "steps = csv.shape[0]\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(steps):\n",
    "        x_var = torch.tensor(inputs.loc[j].values).float()\n",
    "        y_var = torch.tensor([targets.loc[j]]).float()          \n",
    "\n",
    "        optimizer.zero_grad() # empty (zero) the gradient buffers\n",
    "        y_hat = logreg_clf(x_var) #get the output from the model\n",
    "\n",
    "        loss = loss_function(y_hat, y_var) #calculate the loss\n",
    "        loss.backward() #backprop\n",
    "        optimizer.step() #does the update\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print (\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw4yfGoGuChe"
   },
   "source": [
    "Run the below code to test the accuracy of your model on the training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "L88WmKtMt5gH",
    "outputId": "6083196f-9653-4d11-b100-e614f10a6e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 39, True Negatives: 20\n",
      "False Positives: 9, False Negatives: 11\n",
      "Class specific accuracy of correctly predicting a hit song is 0.78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "test = pd.read_csv(\"https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\")\n",
    "labels = test.iloc[:,-1]\n",
    "test = test.drop('Topclass1030', axis=1)\n",
    "testdata = torch.Tensor(test.values)\n",
    "testlabels = torch.Tensor(labels.values).view(-1,1)\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "\n",
    "for i in range(0, testdata.size()[0]): \n",
    "  # print(testdata[i].size())\n",
    "  Xtest = torch.Tensor(testdata[i])\n",
    "  y_hat = logreg_clf(Xtest)\n",
    "  \n",
    "  if y_hat > 0.5:\n",
    "    prediction = 1\n",
    "  else: \n",
    "    prediction = 0\n",
    "\n",
    "  if (prediction == testlabels[i]):\n",
    "    if (prediction == 1):\n",
    "      TP += 1\n",
    "    else: \n",
    "      TN += 1\n",
    "\n",
    "  else:\n",
    "    if (prediction == 1):\n",
    "      FP += 1\n",
    "    else: \n",
    "      FN += 1\n",
    "\n",
    "print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
    "print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
    "rate = TP/(FN+TP)\n",
    "print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI homework 1 - torch intro",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
