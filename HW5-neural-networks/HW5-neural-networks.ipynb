{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUbEmuvZJxlI"
   },
   "source": [
    "# Homework 5: Neural Networks in PyTorch\n",
    "\n",
    "-- Prof. Dorien Herremans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efS07mO7J6AR"
   },
   "source": [
    "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xDkwBg8LKQ_"
   },
   "source": [
    " ## Question 1 -- XOR neural network [3pts]\n",
    "\n",
    "a) Train an (at least) 2-layer neural network that can solve the XOR problem. \n",
    "\n",
    "b) Check the predictions resulting from your model in the second code box below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:43:19.584467Z",
     "start_time": "2020-07-05T03:43:18.835636Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "BINvhm-PLKak",
    "outputId": "f239dca5-2221-4b9f-971d-6f2dc5fe7d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 0, Loss: 0.8492563366889954\n",
      "Epoch num: 20, Loss: 0.6953948736190796\n",
      "Epoch num: 40, Loss: 0.6758471727371216\n",
      "Epoch num: 60, Loss: 0.6439141631126404\n",
      "Epoch num: 80, Loss: 0.5481469035148621\n",
      "Epoch num: 100, Loss: 0.3501562178134918\n",
      "Epoch num: 120, Loss: 0.1337297111749649\n",
      "Epoch num: 140, Loss: 0.05148107931017876\n",
      "Epoch num: 160, Loss: 0.02359386533498764\n",
      "Epoch num: 180, Loss: 0.014554874040186405\n"
     ]
    }
   ],
   "source": [
    "# load your data\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "train_x = torch.Tensor([[0,0], [1,1], [0,1], [1,0]])\n",
    "train_y = torch.LongTensor([[0], [0], [1], [1]])\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
    "  # hidden_dim: The size of each of the hidden layers.\n",
    "  # dropout: The proportion of units to drop out after each layer.\n",
    "  def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "    super(FeedForwardNN, self).__init__()\n",
    "    \n",
    "    # Set up the hidden layers.\n",
    "    assert num_hidden > 0\n",
    "    # A special ModuleList to store our hidden layers.\n",
    "    self.hidden_layers = nn.ModuleList([])\n",
    "    # First hidden layer maps from input_size -> num_hidden.\n",
    "    self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
    "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
    "    # Note that they can map to any dimensionality --- as long as the final\n",
    "    # output is a distribution over your classes!\n",
    "    for i in range(num_hidden - 1):\n",
    "      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "    \n",
    "    # Set up the dropout layer.\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Set up the final transform to a distribution over classes.\n",
    "    self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    # Set up the nonlinearity to use between layers.\n",
    "    self.nonlinearity = nn.ReLU()\n",
    "    \n",
    "  # Forward's sole argument is the input.\n",
    "  # input is of shape (batch_size, input_size)\n",
    "  def forward(self, x):\n",
    "    # Apply the hidden layers, nonlinearity, and dropout.\n",
    "    for hidden_layer in self.hidden_layers:\n",
    "      x = hidden_layer(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.nonlinearity(x)\n",
    "      \n",
    "    # Output layer: project x to a distribution over classes.\n",
    "    out = self.output_projection(x)    \n",
    "\n",
    "    # Softmax the out tensor to get a log-probability distribution\n",
    "    # over classes for each example.\n",
    "    out_distribution = F.log_softmax(out, dim=-1)\n",
    "\n",
    "    return out_distribution\n",
    "\n",
    "# name your model xor\n",
    "def xor_model():\n",
    "    input_size = 2\n",
    "    num_classes = 2\n",
    "    num_hidden = 2\n",
    "    hidden_dim = 10\n",
    "    dropout = 0.0\n",
    "    return FeedForwardNN(input_size,\n",
    "                         num_classes,\n",
    "                         num_hidden,\n",
    "                         hidden_dim,\n",
    "                         dropout)\n",
    "    \n",
    "xor = xor_model()    \n",
    "\n",
    "# define your model loss function, optimizer, etc. \n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(xor.parameters(),\n",
    "                      lr=lr,\n",
    "                      momentum=momentum)\n",
    "# train the model\n",
    "epoch = 200\n",
    "steps = train_x.size(0)\n",
    "\n",
    "def train(model, optimizer, criterion, x = train_x, y = train_y):\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j in range(steps):\n",
    "            optimizer.zero_grad()             \n",
    "            inp = x[j].unsqueeze(0)\n",
    "            label = y[j].type(torch.LongTensor)      \n",
    "            predicted = model(inp)   \n",
    "            loss = criterion(predicted, label)     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"Epoch num: {}, Loss: {}\".format(i, loss))\n",
    "\n",
    "train(xor, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:43:19.591893Z",
     "start_time": "2020-07-05T03:43:19.586051Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "51Ra1T6n2r_R",
    "outputId": "e848573a-7467-4e81-f397-663f6d5d3f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 xor 0 = 0\n",
      "0 xor 1 = 1\n",
      "1 xor 1 = 0\n",
      "1 xor 0 = 1\n"
     ]
    }
   ],
   "source": [
    "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
    "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
    "\n",
    "test = [[0,0],[0,1],[1,1],[1,0]]\n",
    "\n",
    "for trial in test: \n",
    "  Xtest = torch.Tensor(trial)\n",
    "  y_hat = xor(Xtest)  \n",
    "  prediction = torch.argmax(y_hat, axis=0)\n",
    "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqIqD5ZzyUOW"
   },
   "source": [
    "## Question 2  [2pts]\n",
    "\n",
    "Imagine a neural network model for a multilabel classification task. \n",
    "\n",
    "a) Which loss function should you use?\n",
    "\n",
    "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzye9G18PQ0c"
   },
   "source": [
    "```\n",
    "[your answer here, no coding required]\n",
    "\n",
    "* answer A\n",
    "  - For multilabel classification, you use binary cross entropy loss for every class. For each class, we would have an output node to check if that class label should be assigned to that instance.\n",
    "* answer B\n",
    "  - Add data to the training set\n",
    "  - Add regularisation\n",
    "  - Add early stopping\n",
    "  - Feature selection to decrease number of features\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcceOSnjjSHf"
   },
   "source": [
    "## Question 3 - Improve hit classification [5pts]\n",
    "\n",
    "Remember the hit predicton dataset from last week? \n",
    "\n",
    "a) Improve the model using a multiplayer perceptron. \n",
    "\n",
    "b) Make sure to run your models on the GPU. \n",
    "\n",
    "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:43:41.163776Z",
     "start_time": "2020-07-05T03:43:19.593354Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "t-jkJDTdjSRX",
    "outputId": "5df739ea-54cf-4495-dbed-cb90b86f1624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 0, Epoch loss: 53.76749038696289\n",
      "Epoch num: 50, Epoch loss: 29.815656661987305\n",
      "Epoch num: 100, Epoch loss: 14.081474304199219\n",
      "Epoch num: 150, Epoch loss: 6.413394927978516\n",
      "Epoch num: 200, Epoch loss: 4.9389519691467285\n",
      "Epoch num: 250, Epoch loss: 4.39068603515625\n",
      "Epoch num: 300, Epoch loss: 4.175754547119141\n",
      "Epoch num: 350, Epoch loss: 4.066369533538818\n",
      "Epoch num: 400, Epoch loss: 3.9939205646514893\n",
      "Epoch num: 450, Epoch loss: 3.9588334560394287\n"
     ]
    }
   ],
   "source": [
    "# code your model 1\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# define MLP model\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
    "  # hidden_dim: The size of each of the hidden layers.\n",
    "  # dropout: The proportion of units to drop out after each layer.\n",
    "  def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "    super(MultiLayerPerceptron, self).__init__()\n",
    "    \n",
    "    # Set up the hidden layers.\n",
    "    assert num_hidden > 0\n",
    "    # A special ModuleList to store our hidden layers.\n",
    "    self.hidden_layers = nn.ModuleList([])\n",
    "    # First hidden layer maps from input_size -> num_hidden.\n",
    "    self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
    "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
    "    # Note that they can map to any dimensionality --- as long as the final\n",
    "    # output is a distribution over your classes!\n",
    "    for i in range(num_hidden - 1):\n",
    "      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "    \n",
    "    # Set up the dropout layer.\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Set up the final transform to a distribution over classes.\n",
    "    self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    # Set up the nonlinearity to use between layers.\n",
    "    self.nonlinearity = nn.ReLU()\n",
    "    \n",
    "  # Forward's sole argument is the input.\n",
    "  # input is of shape (batch_size, input_size)\n",
    "  def forward(self, x):\n",
    "    # Apply the hidden layers, nonlinearity, and dropout.\n",
    "    for hidden_layer in self.hidden_layers:\n",
    "      x = hidden_layer(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.nonlinearity(x)\n",
    "      \n",
    "    # Output layer: project x to a distribution over classes.\n",
    "    out = self.output_projection(x)    \n",
    "\n",
    "    # Softmax the out tensor to get a log-probability distribution\n",
    "    # over classes for each example.\n",
    "    out = torch.sigmoid(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class HitPredictionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.inputs[idx].astype(np.float32), self.targets[idx].astype(np.float32))\n",
    "\n",
    "# load data\n",
    "csv = pd.read_csv(\"https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\")\n",
    "inputs = csv.drop('Topclass1030', 1).to_numpy()\n",
    "targets = csv['Topclass1030'].to_numpy()\n",
    "\n",
    "batch_size = 4\n",
    "train_dataset = HitPredictionDataset(inputs, targets)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "# train model 1\n",
    "input_size = csv.shape[1] - 1\n",
    "num_classes = 1\n",
    "num_hidden = 2\n",
    "hidden_dim = 5\n",
    "dropout = 0.0\n",
    "\n",
    "model1 = MultiLayerPerceptron(input_size,\n",
    "                              num_classes,\n",
    "                              num_hidden,\n",
    "                              hidden_dim,\n",
    "                              dropout)\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "lr = 0.002\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model1.parameters(),\n",
    "                      lr=lr,\n",
    "                      momentum=momentum)\n",
    "num_epochs = 500\n",
    "\n",
    "def train(model, num_epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs): \n",
    "        total_batch_loss = 0\n",
    "        for (inputs, targets) in train_dataloader:\n",
    "            predicted = model(inputs).squeeze(1)   \n",
    "            batch_loss = criterion(predicted, targets)\n",
    "            total_batch_loss += batch_loss\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch num: {}, Epoch loss: {}\".format(epoch, total_batch_loss))\n",
    "\n",
    "train(model1, num_epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:43:41.913991Z",
     "start_time": "2020-07-05T03:43:41.165138Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "UIDPTKcFkETc",
    "outputId": "cbac308a-f06f-4964-a2e2-c9776ebbc8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 37, True Negatives: 17\n",
      "False Positives: 12, False Negatives: 13\n",
      "Class specific accuracy of correctly predicting a hit song is 0.74\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 1 (called model1 here)\n",
    "import pandas as pd \n",
    "\n",
    "def run_evaluation(my_model):\n",
    "\n",
    "#   test = pd.read_csv('/content/herremans_hit_1030test.csv')\n",
    "  test = pd.read_csv('https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv')\n",
    "  labels = test.iloc[:,-1]\n",
    "  test = test.drop('Topclass1030', axis=1)\n",
    "  testdata = torch.Tensor(test.values)\n",
    "  testlabels = torch.Tensor(labels.values).view(-1,1)\n",
    "\n",
    "  TP = 0\n",
    "  TN = 0\n",
    "  FN = 0\n",
    "  FP = 0\n",
    "\n",
    "  for i in range(0, testdata.size()[0]): \n",
    "    # print(testdata[i].size())\n",
    "    Xtest = torch.Tensor(testdata[i])\n",
    "    y_hat = my_model(Xtest)\n",
    "    \n",
    "    if y_hat > 0.5:\n",
    "      prediction = 1\n",
    "    else: \n",
    "      prediction = 0\n",
    "\n",
    "    if (prediction == testlabels[i]):\n",
    "      if (prediction == 1):\n",
    "        TP += 1\n",
    "      else: \n",
    "        TN += 1\n",
    "\n",
    "    else:\n",
    "      if (prediction == 1):\n",
    "        FP += 1\n",
    "      else: \n",
    "        FN += 1\n",
    "\n",
    "  print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
    "  print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
    "  rate = TP/(FN+TP)\n",
    "  print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
    "\n",
    "run_evaluation(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:44:02.941282Z",
     "start_time": "2020-07-05T03:43:41.915623Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "xghPDDNmkHn2",
    "outputId": "03e9978e-5629-444c-d79e-697ef5bde62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 0, Epoch loss: 54.760826110839844\n",
      "Epoch num: 50, Epoch loss: 2.1095807552337646\n",
      "Epoch num: 100, Epoch loss: 0.2000361829996109\n",
      "Epoch num: 150, Epoch loss: 0.09303463250398636\n",
      "Epoch num: 200, Epoch loss: 0.057620372623205185\n",
      "Epoch num: 250, Epoch loss: 0.04126419499516487\n",
      "Epoch num: 300, Epoch loss: 0.03147227689623833\n",
      "Epoch num: 350, Epoch loss: 0.025633778423070908\n",
      "Epoch num: 400, Epoch loss: 0.02108062244951725\n",
      "Epoch num: 450, Epoch loss: 0.017952799797058105\n"
     ]
    }
   ],
   "source": [
    "# code your model 2\n",
    "input_size2 = csv.shape[1] - 1\n",
    "num_classes2 = 1\n",
    "num_hidden2 = 2\n",
    "hidden_dim2 = 20\n",
    "dropout2 = 0.0\n",
    "\n",
    "model2 = MultiLayerPerceptron(input_size2,\n",
    "                              num_classes2,\n",
    "                              num_hidden2,\n",
    "                              hidden_dim2,\n",
    "                              dropout2)\n",
    "\n",
    "optimizer2 = optim.SGD(model2.parameters(),\n",
    "                      lr=lr,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train(model2, num_epochs, optimizer2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:44:03.642683Z",
     "start_time": "2020-07-05T03:44:02.942909Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wAIifiHJkHyW",
    "outputId": "8b6a5672-0ec7-41b4-a7ea-9bfa6a3ab1f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 37, True Negatives: 17\n",
      "False Positives: 12, False Negatives: 13\n",
      "Class specific accuracy of correctly predicting a hit song is 0.74\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 2 (called model2 here)\n",
    "run_evaluation(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPsxbT0KkGs1"
   },
   "source": [
    "Which works better and why do you think this may be (very briefly)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GzjI77HkSwH"
   },
   "source": [
    "The model 2 works better than model 1. The main difference between model2 compared to model1 is that for each hidden layers, model2 has 50 hidden dimensions while model1 has only 20 hidden dimensions. Since model2 is a larger model, it reduced avoidable bias caused by underfitting which reduced training loss and improved the performance of the model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI homework 2 - neural networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
